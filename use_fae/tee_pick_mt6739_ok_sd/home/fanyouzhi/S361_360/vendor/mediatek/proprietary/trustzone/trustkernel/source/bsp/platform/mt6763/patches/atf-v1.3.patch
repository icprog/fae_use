diff --git a/Makefile b/Makefile
index 97822eb..dcd656f 100644
--- a/Makefile
+++ b/Makefile
@@ -73,6 +73,9 @@ endif
 ifeq (${SECURE_OS},teeid)
 SPD				:= teeid
 endif
+ifeq (${SECURE_OS},tkcored)
+SPD				:= tkcored
+endif
 # The AArch32 Secure Payload to be built as BL32 image
 AARCH32_SP			:= none
 # Base commit to perform code check on
@@ -214,6 +217,10 @@ ifeq (${ARCH},aarch32)
         LOAD_IMAGE_V2	:=	1
 endif
 
+ifeq (${SECURE_OS},tkcored)
+CFLAGS += -DCFG_TRUSTKERNEL_TEE_SUPPORT=1
+endif
+
 ################################################################################
 # Toolchain
 ################################################################################
diff --git a/include/bl32/tkcore/tkcore.h b/include/bl32/tkcore/tkcore.h
new file mode 100644
index 0000000..25caa9c
--- /dev/null
+++ b/include/bl32/tkcore/tkcore.h
@@ -0,0 +1,174 @@
+#ifndef __TKCORE_H__
+#define __TKCORE_H__
+#include <mtk_plat_common.h>
+
+#define TKCORE_ENTRY_DONE	0xf2000000
+#define TKCORE_ON_DONE		0xf2000001
+#define TKCORE_OFF_DONE		0xf2000002
+#define TKCORE_SUSPEND_DONE	0xf2000003
+#define TKCORE_RESUME_DONE	0xf2000004
+#define TKCORE_PREEMPTED		0xf2000005
+
+#define TKCORE_HANDLED_S_EL1_FIQ	0xf2000006
+#define TKCORE_EL3_FIQ		0xf2000007
+
+/* SMC function ID that TKCORE uses to request service from secure monitor */
+#define TKCORE_GET_ARGS		0xf2001000
+
+/*
+ * Identifiers for various TKCORE services. Corresponding function IDs (whether
+ * fast or standard) are generated by macros defined below
+ */
+#define TKCORE_ADD		0x2000
+#define TKCORE_SUB		0x2001
+#define TKCORE_MUL		0x2002
+#define TKCORE_DIV		0x2003
+#define TKCORE_HANDLE_FIQ_AND_RETURN	0x2004
+
+#define TKCORE_STD_FID(fid)	((fid) | 0x72000000 | (0 << 31))
+#define TKCORE_FAST_FID(fid)	((fid) | 0x72000000 | (1 << 31))
+
+/* SMC function ID to request a previously preempted std smc */
+#define TKCORE_FID_RESUME	TKCORE_STD_FID(0x3000)
+
+/*
+ * Identify a TKCORE service from function ID filtering the last 16 bits from the
+ * SMC function ID
+ */
+#define TKCORE_BARE_FID(fid)	((fid) & 0xffff)
+
+/*
+ * Total number of function IDs implemented for services offered to NS clients.
+ * The function IDs are defined above
+ */
+#define TKCORE_NUM_FID		0x4
+
+/* TKCORE implementation version numbers */
+#define TKCORE_VERSION_MAJOR	0x0 /* Major version */
+#define TKCORE_VERSION_MINOR	0x1 /* Minor version */
+
+/*
+ * Standard Trusted OS Function IDs that fall under Trusted OS call range
+ * according to SMC calling convention
+ */
+#define TOS_CALL_COUNT		0xbf00ff00 /* Number of calls implemented */
+#define TOS_UID			0xbf00ff01 /* Implementation UID */
+/*				0xbf00ff02 is reserved */
+#define TOS_CALL_VERSION	0xbf00ff03 /* Trusted OS Call Version */
+
+/* Definitions to help the assembler access the SMC/ERET args structure */
+#define TKCORE_ARGS_SIZE		0x40
+#define TKCORE_ARG0		0x0
+#define TKCORE_ARG1		0x8
+#define TKCORE_ARG2		0x10
+#define TKCORE_ARG3		0x18
+#define TKCORE_ARG4		0x20
+#define TKCORE_ARG5		0x28
+#define TKCORE_ARG6		0x30
+#define TKCORE_ARG7		0x38
+#define TKCORE_ARGS_END		0x40
+
+#define PLAT_TKCORE_SECURE_SERVICE_SUCCESS	((uint32_t)0)
+#define PLAT_TKCORE_SECURE_SERVICE_ERROR	((uint32_t)-1)
+
+#define PLAT_TKCORE_SECURE_SERVICE_TEST		(0x0)
+#define PLAT_TKCORE_SECURE_SERVICE_WDT_DUMP	(0x1)
+
+#ifndef __ASSEMBLY__
+
+#include <cassert.h>
+#include <platform_def.h> /* For CACHE_WRITEBACK_GRANULE */
+#include <spinlock.h>
+#include <stdint.h>
+
+typedef uint32_t tkcore_vector_isn_t;
+
+typedef struct tkcore_vectors {
+	tkcore_vector_isn_t std_smc_entry;
+	tkcore_vector_isn_t fast_smc_entry;
+	tkcore_vector_isn_t cpu_on_entry;
+	tkcore_vector_isn_t cpu_off_entry;
+	tkcore_vector_isn_t cpu_resume_entry;
+	tkcore_vector_isn_t cpu_suspend_entry;
+	tkcore_vector_isn_t fiq_entry;
+} tkcore_vectors_t;
+
+typedef struct work_statistics {
+	uint32_t fiq_count;		/* Number of FIQs on this cpu */
+	uint32_t irq_count;		/* Number of IRQs on this cpu */
+	uint32_t sync_fiq_count;	/* Number of sync. fiqs on this cpu */
+	uint32_t sync_fiq_ret_count;	/* Number of fiq returns on this cpu */
+	uint32_t smc_count;		/* Number of returns on this cpu */
+	uint32_t eret_count;		/* Number of entries on this cpu */
+	uint32_t cpu_on_count;		/* Number of cpu on requests */
+	uint32_t cpu_off_count;		/* Number of cpu off requests */
+	uint32_t cpu_suspend_count;	/* Number of cpu suspend requests */
+	uint32_t cpu_resume_count;	/* Number of cpu resume requests */
+} __aligned(CACHE_WRITEBACK_GRANULE) work_statistics_t;
+
+typedef struct tkcore_args {
+	uint64_t _regs[TKCORE_ARGS_END >> 3];
+} __aligned(CACHE_WRITEBACK_GRANULE) tkcore_args_t;
+
+/* Macros to access members of the above structure using their offsets */
+#define read_sp_arg(args, offset)	((args)->_regs[offset >> 3])
+#define write_sp_arg(args, offset, val) (((args)->_regs[offset >> 3])	\
+					 = val)
+
+/*
+ * Ensure that the assembler's view of the size of the tkcore_args is the
+ * same as the compilers
+ */
+CASSERT(TKCORE_ARGS_SIZE == sizeof(tkcore_args_t), assert_sp_args_size_mismatch);
+
+void tkcore_get_magic(uint64_t args[4]);
+
+tkcore_args_t *tkcore_cpu_resume_main(uint64_t arg0,
+				uint64_t arg1,
+				uint64_t arg2,
+				uint64_t arg3,
+				uint64_t arg4,
+				uint64_t arg5,
+				uint64_t arg6,
+				uint64_t arg7);
+tkcore_args_t *tkcore_cpu_suspend_main(uint64_t arg0,
+				 uint64_t arg1,
+				 uint64_t arg2,
+				 uint64_t arg3,
+				 uint64_t arg4,
+				 uint64_t arg5,
+				 uint64_t arg6,
+				 uint64_t arg7);
+tkcore_args_t *tkcore_cpu_on_main(void);
+tkcore_args_t *tkcore_cpu_off_main(uint64_t arg0,
+			     uint64_t arg1,
+			     uint64_t arg2,
+			     uint64_t arg3,
+			     uint64_t arg4,
+			     uint64_t arg5,
+			     uint64_t arg6,
+			     uint64_t arg7);
+
+/* Generic Timer functions */
+void tkcore_generic_timer_start(void);
+void tkcore_generic_timer_handler(void);
+void tkcore_generic_timer_stop(void);
+void tkcore_generic_timer_save(void);
+void tkcore_generic_timer_restore(void);
+
+/* FIQ management functions */
+void tkcore_update_sync_fiq_stats(uint32_t type, uint64_t elr_el3);
+
+/* Data structure to keep track of TKCORE statistics */
+extern spinlock_t console_lock;
+extern work_statistics_t tkcore_stats[PLATFORM_CORE_COUNT];
+
+/* Vector table of jumps */
+extern tkcore_vectors_t tkcore_vector_table;
+
+uint32_t plat_tkcore_secure_service_request(uint64_t service_id, uint64_t *x2, uint64_t *x3);
+extern void plat_tkcore_dump(void);
+
+#endif /* __ASSEMBLY__ */
+
+#endif
diff --git a/plat/mediatek/common/log.c b/plat/mediatek/common/log.c
index e9bd0ba..c92c6d5 100644
--- a/plat/mediatek/common/log.c
+++ b/plat/mediatek/common/log.c
@@ -376,6 +376,7 @@ void mt_log_setup(unsigned int start, unsigned int size, unsigned int aee_buf_si
 
 #define MT_LOG_SECURE_OS_BUFFER_MAX_LENGTH 120
 #define TBASE_TAG "TBASE"
+#define TKCORE_TAG "TKCORE"
 static unsigned char mt_log_secure_os_buf[MT_LOG_SECURE_OS_BUFFER_MAX_LENGTH+1] = {0};
 static unsigned int mt_log_secure_os_pos = 0;
 
@@ -389,6 +390,8 @@ void mt_log_secure_os_print(int c)
 		mt_log_secure_os_buf[mt_log_secure_os_pos+1] = '\0';
 #if CFG_MICROTRUST_TEE_SUPPORT
 	NOTICE("[%s]%s", "uTos", mt_log_secure_os_buf);
+#elif CFG_TRUSTKERNEL_TEE_SUPPORT
+	NOTICE("[%s]%s", TKCORE_TAG, mt_log_secure_os_buf);
 #else
 	NOTICE("[%s]%s", TBASE_TAG, mt_log_secure_os_buf);
 #endif
@@ -404,8 +407,9 @@ void mt_log_secure_os_print(int c)
 		mt_log_secure_os_buf[mt_log_secure_os_pos] = '\0';
 #if CFG_MICROTRUST_TEE_SUPPORT
 	NOTICE("[%s]%s", "uTos", mt_log_secure_os_buf);
+#elif CFG_TRUSTKERNEL_TEE_SUPPORT
+	NOTICE("[%s]%s", TKCORE_TAG, mt_log_secure_os_buf);
 #else
-
 	NOTICE("[%s]%s", TBASE_TAG, mt_log_secure_os_buf);
 #endif
 		mt_log_secure_os_pos = 0;
diff --git a/plat/mediatek/mt6763/drivers/devapc/devapc.c b/plat/mediatek/mt6763/drivers/devapc/devapc.c
index 9a93f0a..9b9a226 100644
--- a/plat/mediatek/mt6763/drivers/devapc/devapc.c
+++ b/plat/mediatek/mt6763/drivers/devapc/devapc.c
@@ -125,7 +125,11 @@ const static INFRA_PERI_DEVICE_INFO D_APC_INFRA_Devices_TEE[] = {
 	DAPC_INFRA_ATTR("PERISYS_I2C0",                          E_NO_PROTECTION,        E_FORBIDDEN             ),
 	DAPC_INFRA_ATTR("PERISYS_I2C1",                          E_NO_PROTECTION,        E_FORBIDDEN             ),
 	DAPC_INFRA_ATTR("PERISYS_I2C2",                          E_NO_PROTECTION,        E_FORBIDDEN             ),
+#if defined(SPD_tkcored)
+	DAPC_INFRA_ATTR("PERISYS_SPI0",                          E_SEC_RW_ONLY,        E_FORBIDDEN             ),
+#else
 	DAPC_INFRA_ATTR("PERISYS_SPI0",                          E_NO_PROTECTION,        E_FORBIDDEN             ),
+#endif
 	DAPC_INFRA_ATTR("PERISYS_PTP",                           E_NO_PROTECTION,        E_NO_PROTECTION         ),
 	DAPC_INFRA_ATTR("PERISYS_BTIF",                          E_NO_PROTECTION,        E_FORBIDDEN             ),
 	DAPC_INFRA_ATTR("PERISYS_IRTX",                          E_NO_PROTECTION,        E_FORBIDDEN             ),
diff --git a/plat/mediatek/mt6763/plat_mt_gic.c b/plat/mediatek/mt6763/plat_mt_gic.c
index aac43e7..4398661 100644
--- a/plat/mediatek/mt6763/plat_mt_gic.c
+++ b/plat/mediatek/mt6763/plat_mt_gic.c
@@ -238,7 +238,7 @@ static
 	while (gicd_v3_read_ctlr(gicd_base) & GICD_V3_CTLR_RWP);
 }
 
-#if  CFG_MICROTRUST_TEE_SUPPORT
+#if  CFG_MICROTRUST_TEE_SUPPORT || CFG_TRUSTKERNEL_TEE_SUPPORT
 #else
 static
 #endif
diff --git a/plat/mediatek/mt6763/plat_tkcore.c b/plat/mediatek/mt6763/plat_tkcore.c
new file mode 100644
index 0000000..d6107c4
--- /dev/null
+++ b/plat/mediatek/mt6763/plat_tkcore.c
@@ -0,0 +1,95 @@
+/*
+ * Copyright (c) 2014 TRUSTONIC LIMITED
+ * All rights reserved
+ *
+ * The present software is the confidential and proprietary information of
+ * TRUSTONIC LIMITED. You shall not disclose the present software and shall
+ * use it only in accordance with the terms of the license agreement you
+ * entered into with TRUSTONIC LIMITED. This software may be subject to
+ * export or import laws in certain countries.
+ */
+
+#include <assert.h>
+#include <debug.h>
+#include <string.h>
+#include <stdio.h>
+#include <platform.h>
+#include <mmio.h>
+#include <platform_def.h>
+#include <console.h>
+#include "plat_private.h"
+#include "tkcore.h"
+
+#include <arch_helpers.h>
+
+#include <gic_v2.h>
+#include <gic_v3.h>
+#include <interrupt_mgmt.h>
+
+int gic_populate_rdist(unsigned int *rdist_base);
+void irq_raise_softirq(unsigned int map, unsigned int irq);
+
+static void tkcore_platform_dump(void);
+
+extern void aee_wdt_dump();
+/* Code */
+uint32_t plat_tkcore_secure_service_request(uint64_t service_id, uint64_t *x2, uint64_t *x3)
+{
+	uint32_t rc = PLAT_TKCORE_SECURE_SERVICE_SUCCESS;
+	uint32_t linear_id = platform_get_core_pos(read_mpidr());
+
+	(void) x2;
+	(void) x3;
+	switch (service_id) {
+		case PLAT_TKCORE_SECURE_SERVICE_TEST:
+			console_init(gteearg.atf_log_port, UART_CLOCK, UART_BAUDRATE);
+			tf_printf("%s core 0x%x service PLAT_TKCORE_SECURE_SERVICE_TEST\n",
+				__func__, (int) linear_id);
+			console_uninit();
+			break;
+		case PLAT_TKCORE_SECURE_SERVICE_WDT_DUMP:
+			tkcore_platform_dump();
+			break;
+		default:
+			rc = PLAT_TKCORE_SECURE_SERVICE_ERROR;
+	}
+
+	return rc;
+}
+
+void plat_tkcore_dump(void)
+{
+	aee_wdt_dump();
+}
+
+static void tkcore_platform_dump(void)
+{
+	uint64_t mpidr = read_mpidr();
+	uint32_t linear_id = platform_get_core_pos(mpidr);
+
+	uint32_t SGITargets;
+	uint32_t rdist_sgi_base = 0;
+
+	/* get the base of redistributor first */
+	if (gic_populate_rdist(&rdist_sgi_base) == -1) {
+		return;
+	}
+	/* Configure SGI */
+	gicd_clr_igroupr(rdist_sgi_base, FIQ_SMP_CALL_SGI);
+	gicd_set_ipriorityr(rdist_sgi_base, FIQ_SMP_CALL_SGI, GIC_HIGHEST_SEC_PRIORITY);
+
+	/* Enable SGI */
+	gicd_set_isenabler(rdist_sgi_base, FIQ_SMP_CALL_SGI);
+
+	/* Send SGIs to all cores except the current one
+	   (current will directly branch to the dump handler) */
+	SGITargets = 0xFF;
+	SGITargets &= ~(1 << linear_id);
+
+	/* Trigger SGI */
+	irq_raise_softirq(SGITargets, FIQ_SMP_CALL_SGI);
+
+	/* Current core directly branches to dump handler */
+	plat_tkcore_dump();
+}
+
diff --git a/plat/mediatek/mt6763/platform.mk b/plat/mediatek/mt6763/platform.mk
index ceedf84..72fe0ed 100644
--- a/plat/mediatek/mt6763/platform.mk
+++ b/plat/mediatek/mt6763/platform.mk
@@ -139,6 +139,9 @@ ifeq (${SPD}, teeid)
 BL31_SOURCE_DRAM += ${MTK_PLAT_SOC}/plat_teei.c
 endif
 
+ifeq (${SPD}, tkcored)
+BL31_SOURCES             +=      ${MTK_PLAT_SOC}/plat_tkcore.c
+endif
 # Flag used by the MTK_platform port to determine the version of ARM GIC
 # architecture to use for interrupt management in EL3.
 ARM_GIC_ARCH := 2
diff --git a/services/spd/tkcored/teesmc.h b/services/spd/tkcored/teesmc.h
new file mode 100644
index 0000000..239b0ad
--- /dev/null
+++ b/services/spd/tkcored/teesmc.h
@@ -0,0 +1,642 @@
+
+#ifndef TEESMC_H
+#define TEESMC_H
+
+#ifndef ASM
+
+/*
+ * Same values as TEE_PARAM_* from TEE Internal API
+ */
+#define TEESMC_ATTR_TYPE_NONE		0
+#define TEESMC_ATTR_TYPE_VALUE_INPUT	1
+#define TEESMC_ATTR_TYPE_VALUE_OUTPUT	2
+#define TEESMC_ATTR_TYPE_VALUE_INOUT	3
+#define TEESMC_ATTR_TYPE_MEMREF_INPUT	5
+#define TEESMC_ATTR_TYPE_MEMREF_OUTPUT	6
+#define TEESMC_ATTR_TYPE_MEMREF_INOUT	7
+
+#define TEESMC_ATTR_TYPE_MASK		0x7
+
+/*
+ * Meta parameter to be absorbed by the Secure OS and not passed
+ * to the Trusted Application.
+ *
+ * One example of this is a struct teesmc_meta_open_session which
+ * is added to TEESMC{32,64}_CMD_OPEN_SESSION.
+ */
+#define TEESMC_ATTR_META		0x8
+
+/*
+ * Used as an indication from normal world of compatible cache usage.
+ * 'I' stands for inner cache and 'O' for outer cache.
+ */
+#define TEESMC_ATTR_CACHE_I_NONCACHE	0x0
+#define TEESMC_ATTR_CACHE_I_WRITE_THR	0x1
+#define TEESMC_ATTR_CACHE_I_WRITE_BACK	0x2
+#define TEESMC_ATTR_CACHE_O_NONCACHE	0x0
+#define TEESMC_ATTR_CACHE_O_WRITE_THR	0x4
+#define TEESMC_ATTR_CACHE_O_WRITE_BACK	0x8
+
+#define TEESMC_ATTR_CACHE_DEFAULT	(TEESMC_ATTR_CACHE_I_WRITE_BACK | \
+					 TEESMC_ATTR_CACHE_O_WRITE_BACK)
+
+#define TEESMC_ATTR_CACHE_SHIFT		4
+#define TEESMC_ATTR_CACHE_MASK		0xf
+
+#define TEESMC_CMD_OPEN_SESSION		0
+#define TEESMC_CMD_INVOKE_COMMAND	1
+#define TEESMC_CMD_CLOSE_SESSION	2
+#define TEESMC_CMD_CANCEL		3
+
+/**
+ * struct teesmc32_param_memref - memory reference
+ * @buf_ptr: Address of the buffer
+ * @size: Size of the buffer
+ *
+ * Secure and normal world communicates pointer via physical address instead of
+ * the virtual address with is usually used for pointers. This is because
+ * Secure and normal world has completely independant memory mapping. Normal
+ * world can even have a hypervisor which need to translate the guest
+ * physical address (AKA IPA in ARM lingo) to a real physical address
+ * before passing the structure to secure world.
+ */
+struct teesmc32_param_memref {
+	uint32_t buf_ptr;
+	uint32_t size;
+};
+
+/**
+ * struct teesmc32_param_memref - memory reference
+ * @buf_ptr: Address of the buffer
+ * @size: Size of the buffer
+ *
+ * See description of struct teesmc32_param_memref.
+ */
+struct teesmc64_param_memref {
+	uint64_t buf_ptr;
+	uint64_t size;
+};
+
+/**
+ * struct teesmc32_param_value - values
+ * @a: first value
+ * @b: second value
+ */
+struct teesmc32_param_value {
+	uint32_t a;
+	uint32_t b;
+};
+
+/**
+ * struct teesmc64_param_value - values
+ * @a: first value
+ * @b: second value
+ */
+struct teesmc64_param_value {
+	uint64_t a;
+	uint64_t b;
+};
+
+/**
+ * struct teesmc32_param - parameter
+ * @attr: attributes
+ * @memref: a memory reference
+ * @value: a value
+ *
+ * attr & TEESMC_ATTR_TYPE_MASK indicates if memref or value is used in the
+ * union. TEESMC_ATTR_TYPE_VALUE_* indicates value and
+ * TEESMC_ATTR_TYPE_MEMREF_* indicates memref. TEESMC_ATTR_TYPE_NONE
+ * indicates that none of the members are used.
+ */
+struct teesmc32_param {
+	uint32_t attr;
+	union {
+		struct teesmc32_param_memref memref;
+		struct teesmc32_param_value value;
+	} u;
+};
+
+/**
+ * struct teesmc64_param - parameter
+ * @attr: attributes
+ * @memref: a memory reference
+ * @value: a value
+ *
+ * See description of union teesmc32_param.
+ */
+struct teesmc64_param {
+	uint64_t attr;
+	union {
+		struct teesmc64_param_memref memref;
+		struct teesmc64_param_value value;
+	} u;
+};
+
+/**
+ * struct teesmc32_arg - SMC argument for Trusted OS
+ * @cmd: Command, one of TEESMC_CMD_*
+ * @ta_func: Trusted Application function, specific to the Trusted Application,
+ *	     used if cmd == TEESMC_CMD_INVOKE_COMMAND
+ * @session: In parameter for all TEESMC_CMD_* except
+ *	     TEESMC_CMD_OPEN_SESSION where it's an output paramter instead
+ * @ret: return value
+ * @ret_origin: origin of the return value
+ * @num_params: number of parameters supplied to the OS Command
+ * @params: the parameters supplied to the OS Command
+ *
+ * All normal SMC calls to Trusted OS uses this struct. If cmd requires
+ * further information than what these field holds it can be passed as a
+ * parameter tagged as meta (setting the TEESMC_ATTR_META bit in
+ * corresponding param_attrs). This is used for TEESMC_CMD_OPEN_SESSION
+ * to pass a struct teesmc32_meta_open_session which is needed find the
+ * Trusted Application and to indicate the credentials of the client.
+ */
+struct teesmc32_arg {
+	uint32_t cmd;
+	uint32_t ta_func;
+	uint32_t session;
+	uint32_t ret;
+	uint32_t ret_origin;
+	uint32_t num_params;
+#if 0
+	/*
+	 * Commented out elements used to visualize the layout dynamic part
+	 * of the struct. Note that these fields are not available at all
+	 * if num_params == 0.
+	 *
+	 * params is accessed through the macro TEESMC32_GET_PARAMS
+	 */
+
+	struct teesmc32_param params[num_params];
+#endif
+};
+
+/**
+ * TEESMC32_GET_PARAMS - return pointer to union teesmc32_param *
+ *
+ * @x: Pointer to a struct teesmc32_arg
+ *
+ * Returns a pointer to the params[] inside a struct teesmc32_arg.
+ */
+#define TEESMC32_GET_PARAMS(x) \
+	(struct teesmc32_param *)(((struct teesmc32_arg *)(x)) + 1)
+
+/**
+ * TEESMC32_GET_ARG_SIZE - return size of struct teesmc32_arg
+ *
+ * @num_params: Number of parameters embedded in the struct teesmc32_arg
+ *
+ * Returns the size of the struct teesmc32_arg together with the number
+ * of embedded paramters.
+ */
+#define TEESMC32_GET_ARG_SIZE(num_params) \
+	(sizeof(struct teesmc32_arg) + \
+	 sizeof(struct teesmc32_param) * (num_params))
+
+/**
+ * struct teesmc64_arg - SMC argument for Trusted OS
+ * @cmd: OS Command, one of TEESMC_CMD_*
+ * @ta_func: Trusted Application function, specific to the Trusted Application
+ * @session: In parameter for all TEESMC_CMD_* but
+ *	     TEESMC_CMD_OPEN_SESSION
+ * @ret: return value
+ * @ret_origin: origin of the return value
+ * @num_params: number of parameters supplied to the OS Command
+ * @params: the parameters supplied to the OS Command
+ *
+ * See description of struct teesmc32_arg.
+ */
+struct teesmc64_arg {
+	uint64_t cmd;
+	uint64_t ta_func;
+	uint64_t session;
+	uint64_t ret;
+	uint64_t ret_origin;
+	uint64_t num_params;
+#if 0
+	/*
+	 * Commented out elements used to visualize the layout dynamic part
+	 * of the struct. Note that these fields are not available at all
+	 * if num_params == 0.
+	 *
+	 * params is accessed through the macro TEESMC64_GET_PARAMS
+	 */
+
+	union teesmc64_param params[num_params];
+#endif
+};
+
+/**
+ * TEESMC64_GET_PARAMS - return pointer to union teesmc64_param *
+ *
+ * @x: Pointer to a struct teesmc64_arg
+ *
+ * Returns a pointer to the params[] inside a struct teesmc64_arg.
+ */
+#define TEESMC64_GET_PARAMS(x) \
+	(struct teesmc64_param *)(((struct teesmc64_arg *)(x)) + 1)
+
+/**
+ * TEESMC64_GET_ARG_SIZE - return size of struct teesmc64_arg
+ *
+ * @num_params: Number of parameters embedded in the struct teesmc64_arg
+ *
+ * Returns the size of the struct teesmc64_arg together with the number
+ * of embedded paramters.
+ */
+#define TEESMC64_GET_ARG_SIZE(num_params) \
+	(sizeof(struct teesmc64_arg) + \
+	 sizeof(union teesmc64_param) * (num_params))
+
+#define TEESMC_UUID_LEN	16
+
+/**
+ * struct teesmc_meta_open_session - additional parameters for
+ *				     TEESMC32_CMD_OPEN_SESSION and
+ *				     TEESMC64_CMD_OPEN_SESSION
+ * @uuid: UUID of the Trusted Application
+ * @clnt_uuid: UUID of client
+ * @clnt_login: Login class of client, TEE_LOGIN_* if being Global Platform
+ *		compliant
+ *
+ * This struct is passed in the first parameter as an input memref tagged
+ * as meta on an TEESMC{32,64}_CMD_OPEN_SESSION cmd. It's important
+ * that it really is the first parameter to make it easy for an eventual
+ * hypervisor to inspect and possibly update clnt_* values.
+ */
+struct teesmc_meta_open_session {
+	uint8_t uuid[TEESMC_UUID_LEN];
+	uint8_t clnt_uuid[TEESMC_UUID_LEN];
+	uint32_t clnt_login;
+};
+
+
+#endif /*!ASM*/
+
+/*
+ *******************************************************************************
+ * Part 2 - low level SMC interaction
+ *******************************************************************************
+ */
+
+#define TEESMC_32			0
+#define TEESMC_64			0x40000000
+#define TEESMC_FAST_CALL		0x80000000
+#define TEESMC_STD_CALL			0
+
+#define TEESMC_OWNER_MASK		0x3F
+#define TEESMC_OWNER_SHIFT		24
+
+#define TEESMC_FUNC_MASK		0xFFFF
+
+#define TEESMC_IS_FAST_CALL(smc_val)	((smc_val) & TEESMC_FAST_CALL)
+#define TEESMC_IS_64(smc_val)		((smc_val) & TEESMC_64)
+#define TEESMC_FUNC_NUM(smc_val)	((smc_val) & TEESMC_FUNC_MASK)
+#define TEESMC_OWNER_NUM(smc_val)	(((smc_val) >> TEESMC_OWNER_SHIFT) & \
+					 TEESMC_OWNER_MASK)
+
+#define TEESMC_CALL_VAL(type, calling_convention, owner, func_num) \
+			((type) | (calling_convention) | \
+			(((owner) & TEESMC_OWNER_MASK) << TEESMC_OWNER_SHIFT) |\
+			((func_num) & TEESMC_FUNC_MASK))
+
+#define TEESMC_OWNER_ARCH		0
+#define TEESMC_OWNER_CPU		1
+#define TEESMC_OWNER_SIP		2
+#define TEESMC_OWNER_OEM		3
+#define TEESMC_OWNER_STANDARD		4
+#define TEESMC_OWNER_TRUSTED_APP	48
+#define TEESMC_OWNER_TRUSTED_OS		50
+
+#define TEESMC_OWNER_TRUSTED_OS_TKCORED	62
+#define TEESMC_OWNER_TRUSTED_OS_API	63
+
+/*
+ * Function specified by SMC Calling convention.
+ */
+#define TEESMC32_FUNCID_CALLS_COUNT	0xFF00
+#define TEESMC32_CALLS_COUNT \
+	TEESMC_CALL_VAL(TEESMC_32, TEESMC_FAST_CALL, \
+			TEESMC_OWNER_TRUSTED_OS_API, \
+			TEESMC32_FUNCID_CALLS_COUNT)
+
+/*
+ * Function specified by SMC Calling convention
+ *
+ * Return one of the following UIDs if using API specified in this file
+ * without further extentions:
+ * 65cb6b93-af0c-4617-8ed6-644a8d1140f8 : Only 32 bit calls are supported
+ * 65cb6b93-af0c-4617-8ed6-644a8d1140f9 : Both 32 and 64 bit calls are supported
+ */
+#define TEESMC_UID_R0			0x65cb6b93
+#define TEESMC_UID_R1			0xaf0c4617
+#define TEESMC_UID_R2			0x8ed6644a
+#define TEESMC_UID32_R3			0x8d1140f8
+#define TEESMC_UID64_R3			0x8d1140f9
+#define TEESMC32_FUNCID_CALLS_UID	0xFF01
+#define TEESMC32_CALLS_UID \
+	TEESMC_CALL_VAL(TEESMC_32, TEESMC_FAST_CALL, \
+			TEESMC_OWNER_TRUSTED_OS_API, \
+			TEESMC32_FUNCID_CALLS_UID)
+
+/*
+ * Function specified by SMC Calling convention
+ *
+ * Returns 1.0 if using API specified in this file without further extentions.
+ */
+#define TEESMC_REVISION_MAJOR	1
+#define TEESMC_REVISION_MINOR	0
+#define TEESMC32_FUNCID_CALLS_REVISION	0xFF03
+#define TEESMC32_CALLS_REVISION \
+	TEESMC_CALL_VAL(TEESMC_32, TEESMC_FAST_CALL, \
+			TEESMC_OWNER_TRUSTED_OS_API, \
+			TEESMC32_FUNCID_CALLS_REVISION)
+
+/*
+ * Get UUID of Trusted OS.
+ *
+ * Used by non-secure world to figure out which Trusted OS is installed.
+ * Note that returned UUID is the UUID of the Trusted OS, not of the API.
+ *
+ * Returns UUID in r0-4/w0-4 in the same way as TEESMC32_CALLS_UID
+ * described above.
+ */
+#define TEESMC_FUNCID_GET_OS_UUID	0
+#define TEESMC32_CALL_GET_OS_UUID \
+	TEESMC_CALL_VAL(TEESMC_32, TEESMC_FAST_CALL, TEESMC_OWNER_TRUSTED_OS, \
+			TEESMC_FUNCID_GET_OS_UUID)
+
+/*
+ * Get revision of Trusted OS.
+ *
+ * Used by non-secure world to figure out which version of the Trusted OS
+ * is installed. Note that the returned revision is the revision of the
+ * Trusted OS, not of the API.
+ *
+ * Returns revision in r0-1/w0-1 in the same way as TEESMC32_CALLS_REVISION
+ * described above.
+ */
+#define TEESMC_FUNCID_GET_OS_REVISION	1
+#define TEESMC32_CALL_GET_OS_REVISION \
+	TEESMC_CALL_VAL(TEESMC_32, TEESMC_FAST_CALL, TEESMC_OWNER_TRUSTED_OS, \
+			TEESMC_FUNCID_GET_OS_REVISION)
+
+
+
+/*
+ * Call with struct teesmc32_arg as argument
+ *
+ * Call register usage:
+ * r0/x0	SMC Function ID, TEESMC32_CALL_WITH_ARG
+ * r1/x1	Physical pointer to a struct teesmc32_arg
+ * r2-6/x2-6	Not used
+ * r7/x7	Hypervisor Client ID register
+ *
+ * Normal return register usage:
+ * r0/x0	Return value, TEESMC_RETURN_*
+ * r1-3/x1-3	Not used
+ * r4-7/x4-7	Preserved
+ *
+ * Ebusy return register usage:
+ * r0/x0	Return value, TEESMC_RETURN_EBUSY
+ * r1-3/x1-3	Preserved
+ * r4-7/x4-7	Preserved
+ *
+ * RPC return register usage:
+ * r0/x0	Return value, TEESMC_RETURN_IS_RPC(val)
+ * r1-2/x1-2	RPC parameters
+ * r3-7/x3-7	Resume information, must be preserved
+ *
+ * Possible return values:
+ * TEESMC_RETURN_UNKNOWN_FUNCTION	Trusted OS does not recognize this
+ *					function.
+ * TEESMC_RETURN_OK			Call completed, result updated in
+ *					the previously supplied struct
+ *					teesmc32_arg.
+ * TEESMC_RETURN_EBUSY			Trusted OS busy, try again later.
+ * TEESMC_RETURN_EBADADDR		Bad physcial pointer to struct
+ *					teesmc32_arg.
+ * TEESMC_RETURN_EBADCMD		Bad/unknown cmd in struct teesmc32_arg
+ * TEESMC_RETURN_IS_RPC()		Call suspended by RPC call to normal
+ *					world.
+ */
+#define TEESMC_FUNCID_CALL_WITH_ARG	2
+#define TEESMC32_CALL_WITH_ARG \
+	TEESMC_CALL_VAL(TEESMC_32, TEESMC_STD_CALL, TEESMC_OWNER_TRUSTED_OS, \
+	TEESMC_FUNCID_CALL_WITH_ARG)
+/* Same as TEESMC32_CALL_WITH_ARG but a "fast call". */
+#define TEESMC32_FASTCALL_WITH_ARG \
+	TEESMC_CALL_VAL(TEESMC_32, TEESMC_FAST_CALL, TEESMC_OWNER_TRUSTED_OS, \
+	TEESMC_FUNCID_CALL_WITH_ARG)
+
+/*
+ * Call with struct teesmc64_arg as argument
+ *
+ * See description of TEESMC32_CALL_WITH_ARG above, uses struct
+ * teesmc64_arg in x1 instead.
+ */
+#define TEESMC64_CALL_WITH_ARG \
+	TEESMC_CALL_VAL(TEESMC_64, TEESMC_STD_CALL, TEESMC_OWNER_TRUSTED_OS, \
+	TEESMC_FUNCID_CALL_WITH_ARG)
+/* Same as TEESMC64_CALL_WITH_ARG but a "fast call". */
+#define TEESMC64_FASTCALL_WITH_ARG \
+	TEESMC_CALL_VAL(TEESMC_64, TEESMC_FAST_CALL, TEESMC_OWNER_TRUSTED_OS, \
+	TEESMC_FUNCID_CALL_WITH_ARG)
+
+/*
+ * Resume from RPC (for example after processing an IRQ)
+ *
+ * Call register usage:
+ * r0/x0	SMC Function ID,
+ *		TEESMC32_CALL_RETURN_FROM_RPC or
+ *		TEESMC32_FASTCALL_RETURN_FROM_RPC
+ * r1-3/x1-3	Value of r1-3/x1-3 when TEESMC32_CALL_WITH_ARG returned
+ *		TEESMC_RETURN_RPC in r0/x0
+ *
+ * Return register usage is the same as for TEESMC32_CALL_WITH_ARG above.
+ *
+ * Possible return values
+ * TEESMC_RETURN_UNKNOWN_FUNCTION	Trusted OS does not recognize this
+ *					function.
+ * TEESMC_RETURN_OK			Original call completed, result
+ *					updated in the previously supplied.
+ *					struct teesmc32_arg
+ * TEESMC_RETURN_RPC			Call suspended by RPC call to normal
+ *					world.
+ * TEESMC_RETURN_EBUSY			Trusted OS busy, try again later.
+ * TEESMC_RETURN_ERESUME		Resume failed, the opaque resume
+ *					information was corrupt.
+ */
+#define TEESMC_FUNCID_RETURN_FROM_RPC	3
+#define TEESMC32_CALL_RETURN_FROM_RPC \
+	TEESMC_CALL_VAL(TEESMC_32, TEESMC_STD_CALL, TEESMC_OWNER_TRUSTED_OS, \
+			TEESMC_FUNCID_RETURN_FROM_RPC)
+/* Same as TEESMC32_CALL_RETURN_FROM_RPC but a "fast call". */
+#define TEESMC32_FASTCALL_RETURN_FROM_RPC \
+	TEESMC_CALL_VAL(TEESMC_32, TEESMC_STD_CALL, TEESMC_OWNER_TRUSTED_OS, \
+			TEESMC_FUNCID_RETURN_FROM_RPC)
+
+/*
+ * Resume from RPC (for example after processing an IRQ)
+ *
+ * See description of TEESMC32_CALL_RETURN_FROM_RPC above, used when
+ * it's a 64bit call that has returned.
+ */
+#define TEESMC64_CALL_RETURN_FROM_RPC \
+	TEESMC_CALL_VAL(TEESMC_64, TEESMC_STD_CALL, TEESMC_OWNER_TRUSTED_OS, \
+			TEESMC_FUNCID_RETURN_FROM_RPC)
+/* Same as TEESMC64_CALL_RETURN_FROM_RPC but a "fast call". */
+#define TEESMC64_FASTCALL_RETURN_FROM_RPC \
+	TEESMC_CALL_VAL(TEESMC_64, TEESMC_STD_CALL, TEESMC_OWNER_TRUSTED_OS, \
+			TEESMC_FUNCID_RETURN_FROM_RPC)
+
+#define TEESMC_RETURN_RPC_PREFIX_MASK	0xFFFF0000
+#define TEESMC_RETURN_RPC_PREFIX	0xFFFF0000
+#define TEESMC_RETURN_RPC_FUNC_MASK	0x0000FFFF
+
+#define TEESMC_RETURN_GET_RPC_FUNC(ret)	((ret) & TEESMC_RETURN_RPC_FUNC_MASK)
+
+#define TEESMC_RPC_VAL(func)		((func) | TEESMC_RETURN_RPC_PREFIX)
+
+/*
+ * Allocate argument memory for RPC parameter passing.
+ * Argument memory is used to hold a struct teesmc32_arg.
+ *
+ * "Call" register usage:
+ * r0/x0	This value, TEESMC_RETURN_RPC_ALLOC
+ * r1/x1	Size in bytes of required argument memory
+ * r2-7/x2-7	Resume information, must be preserved
+ *
+ * "Return" register usage:
+ * r0/x0	SMC Function ID, TEESMC32_CALL_RETURN_FROM_RPC if it was an
+ *		AArch32 SMC return or TEESMC64_CALL_RETURN_FROM_RPC for
+ *		AArch64 SMC return
+ * r1/x1	Physical pointer to allocated argument memory, 0 if size
+ *		was 0 or if memory can't be allocated
+ * r2-7/x2-7	Preserved
+ */
+#define TEESMC_RPC_FUNC_ALLOC_ARG	0
+#define TEESMC_RETURN_RPC_ALLOC_ARG	\
+	TEESMC_RPC_VAL(TEESMC_RPC_FUNC_ALLOC_ARG)
+
+/*
+ * Allocate payload memory for RPC parameter passing.
+ * Payload memory is used to hold the memory referred to by struct
+ * teesmc32_param_memref.
+ *
+ * "Call" register usage:
+ * r0/x0	This value, TEESMC_RETURN_RPC_ALLOC
+ * r1/x1	Size in bytes of required payload memory
+ * r2-7/x2-7	Resume information, must be preserved
+ *
+ * "Return" register usage:
+ * r0/x0	SMC Function ID, TEESMC32_CALL_RETURN_FROM_RPC if it was an
+ *		AArch32 SMC return or TEESMC64_CALL_RETURN_FROM_RPC for
+ *		AArch64 SMC return
+ * r1/x1	Physical pointer to allocated payload memory, 0 if size
+ *		was 0 or if memory can't be allocated
+ * r2-7/x2-7	Preserved
+ */
+#define TEESMC_RPC_FUNC_ALLOC_PAYLOAD	1
+#define TEESMC_RETURN_RPC_ALLOC_PAYLOAD	\
+	TEESMC_RPC_VAL(TEESMC_RPC_FUNC_ALLOC_PAYLOAD)
+
+/*
+ * Free memory previously allocated by TEESMC_RETURN_RPC_ALLOC_ARG.
+ *
+ * "Call" register usage:
+ * r0/x0	This value, TEESMC_RETURN_RPC_FREE
+ * r1/x1	Physical pointer to previously allocated argument memory
+ * r2-7/x2-7	Resume information, must be preserved
+ *
+ * "Return" register usage:
+ * r0/x0	SMC Function ID, TEESMC32_CALL_RETURN_FROM_RPC if it was an
+ *		AArch32 SMC return or TEESMC64_CALL_RETURN_FROM_RPC for
+ *		AArch64 SMC return
+ * r1/x1	Not used
+ * r2-7/x2-7	Preserved
+ */
+#define TEESMC_RPC_FUNC_FREE_ARG	2
+#define TEESMC_RETURN_RPC_FREE_ARG	TEESMC_RPC_VAL(TEESMC_RPC_FUNC_FREE_ARG)
+
+/*
+ * Free memory previously allocated by TEESMC_RETURN_RPC_ALLOC_PAYLOAD.
+ *
+ * "Call" register usage:
+ * r0/x0	This value, TEESMC_RETURN_RPC_FREE
+ * r1/x1	Physical pointer to previously allocated payload memory
+ * r3-7/x3-7	Resume information, must be preserved
+ *
+ * "Return" register usage:
+ * r0/x0	SMC Function ID, TEESMC32_CALL_RETURN_FROM_RPC if it was an
+ *		AArch32 SMC return or TEESMC64_CALL_RETURN_FROM_RPC for
+ *		AArch64 SMC return
+ * r1-2/x1-2	Not used
+ * r3-7/x3-7	Preserved
+ */
+#define TEESMC_RPC_FUNC_FREE_PAYLOAD	3
+#define TEESMC_RETURN_RPC_FREE_PAYLOAD	\
+	TEESMC_RPC_VAL(TEESMC_RPC_FUNC_FREE_PAYLOAD)
+
+/*
+ * Deliver an IRQ in normal world.
+ *
+ * "Call" register usage:
+ * r0/x0	TEESMC_RETURN_RPC_IRQ
+ * r1-7/x1-7	Resume information, must be preserved
+ *
+ * "Return" register usage:
+ * r0/x0	SMC Function ID, TEESMC32_CALL_RETURN_FROM_RPC if it was an
+ *		AArch32 SMC return or TEESMC64_CALL_RETURN_FROM_RPC for
+ *		AArch64 SMC return
+ * r1-7/x1-7	Preserved
+ */
+#define TEESMC_RPC_FUNC_IRQ		4
+#define TEESMC_RETURN_RPC_IRQ		TEESMC_RPC_VAL(TEESMC_RPC_FUNC_IRQ)
+
+/*
+ * Do an RPC request. The supplied struct teesmc{32,64}_arg tells which
+ * request to do and the paramters for the request. The following fields
+ * are used (the rest are unused):
+ * - cmd		the Request ID
+ * - ret		return value of the request, filled in by normal world
+ * - num_params		number of parameters for the request
+ * - params		the parameters
+ * - param_attrs	attributes of the parameters
+ *
+ * "Call" register usage:
+ * r0/x0	TEESMC_RETURN_RPC_CMD
+ * r1/x1	Physical pointer to a struct teesmc32_arg if returning from
+ *		a AArch32 SMC or a struct teesmc64_arg if returning from a
+ *		AArch64 SMC, must be preserved, only the data should
+ *		be updated
+ * r2-7/x2-7	Resume information, must be preserved
+ *
+ * "Return" register usage:
+ * r0/x0	SMC Function ID, TEESMC32_CALL_RETURN_FROM_RPC if it was an
+ *		AArch32 SMC return or TEESMC64_CALL_RETURN_FROM_RPC for
+ *		AArch64 SMC return
+ * r1-7/x1-7	Preserved
+ */
+#define TEESMC_RPC_FUNC_CMD		5
+#define TEESMC_RETURN_RPC_CMD		TEESMC_RPC_VAL(TEESMC_RPC_FUNC_CMD)
+
+
+/* Returned in r0 */
+#define TEESMC_RETURN_UNKNOWN_FUNCTION	0xFFFFFFFF
+
+/* Returned in r0 only from Trusted OS functions */
+#define TEESMC_RETURN_OK		0x0
+#define TEESMC_RETURN_EBUSY		0x1
+#define TEESMC_RETURN_ERESUME		0x2
+#define TEESMC_RETURN_EBADADDR		0x3
+#define TEESMC_RETURN_EBADCMD		0x4
+#define TEESMC_RETURN_IS_RPC(ret) \
+	(((ret) & TEESMC_RETURN_RPC_PREFIX_MASK) == TEESMC_RETURN_RPC_PREFIX)
+
+/*
+ * Returned in r1 by Trusted OS functions if r0 = TEESMC_RETURN_RPC
+ */
+#define TEESMC_RPC_REQUEST_IRQ		0x0
+
+#endif /* TEESMC_H */
diff --git a/services/spd/tkcored/teesmc_tkcored.h b/services/spd/tkcored/teesmc_tkcored.h
new file mode 100644
index 0000000..48ae981
--- /dev/null
+++ b/services/spd/tkcored/teesmc_tkcored.h
@@ -0,0 +1,89 @@
+#ifndef TEESMC_TKCORED_H
+#define TEESMC_TKCORED_H
+
+#define TEESMC_TKCORED_FUNCID_RETURN_ENTRY_DONE		0
+#define TEESMC32_TKCORED_RETURN_ENTRY_DONE \
+	TEESMC_CALL_VAL(TEESMC_32, TEESMC_FAST_CALL, \
+			TEESMC_OWNER_TRUSTED_OS_TKCORED, \
+			TEESMC_TKCORED_FUNCID_RETURN_ENTRY_DONE)
+#define TEESMC64_TKCORED_RETURN_ENTRY_DONE \
+	TEESMC_CALL_VAL(TEESMC_64, TEESMC_FAST_CALL, \
+			TEESMC_OWNER_TRUSTED_OS_TKCORED, \
+			TEESMC_TKCORED_FUNCID_RETURN_ENTRY_DONE)
+
+
+#define TEESMC_TKCORED_FUNCID_RETURN_ON_DONE		1
+#define TEESMC32_TKCORED_RETURN_ON_DONE \
+	TEESMC_CALL_VAL(TEESMC_32, TEESMC_FAST_CALL, \
+			TEESMC_OWNER_TRUSTED_OS_TKCORED, \
+			TEESMC_TKCORED_FUNCID_RETURN_ON_DONE)
+#define TEESMC64_TKCORED_RETURN_ON_DONE \
+	TEESMC_CALL_VAL(TEESMC_64, TEESMC_FAST_CALL, \
+			TEESMC_OWNER_TRUSTED_OS_TKCORED, \
+			TEESMC_TKCORED_FUNCID_RETURN_ON_DONE)
+
+
+#define TEESMC_TKCORED_FUNCID_RETURN_OFF_DONE		2
+#define TEESMC32_TKCORED_RETURN_OFF_DONE \
+	TEESMC_CALL_VAL(TEESMC_32, TEESMC_FAST_CALL, \
+			TEESMC_OWNER_TRUSTED_OS_TKCORED, \
+			TEESMC_TKCORED_FUNCID_RETURN_OFF_DONE)
+#define TEESMC64_TKCORED_RETURN_OFF_DONE \
+	TEESMC_CALL_VAL(TEESMC_64, TEESMC_FAST_CALL, \
+			TEESMC_OWNER_TRUSTED_OS_TKCORED, \
+			TEESMC_TKCORED_FUNCID_RETURN_OFF_DONE)
+
+
+#define TEESMC_TKCORED_FUNCID_RETURN_SUSPEND_DONE	3
+#define TEESMC32_TKCORED_RETURN_SUSPEND_DONE \
+	TEESMC_CALL_VAL(TEESMC_32, TEESMC_FAST_CALL, \
+			TEESMC_OWNER_TRUSTED_OS_TKCORED, \
+			TEESMC_TKCORED_FUNCID_RETURN_SUSPEND_DONE)
+#define TEESMC64_TKCORED_RETURN_SUSPEND_DONE \
+	TEESMC_CALL_VAL(TEESMC_64, TEESMC_FAST_CALL, \
+			TEESMC_OWNER_TRUSTED_OS_TKCORED, \
+			TEESMC_TKCORED_FUNCID_RETURN_SUSPEND_DONE)
+
+#define TEESMC_TKCORED_FUNCID_RETURN_RESUME_DONE		4
+#define TEESMC32_TKCORED_RETURN_RESUME_DONE \
+	TEESMC_CALL_VAL(TEESMC_32, TEESMC_FAST_CALL, \
+			TEESMC_OWNER_TRUSTED_OS_TKCORED, \
+			TEESMC_TKCORED_FUNCID_RETURN_RESUME_DONE)
+#define TEESMC64_TKCORED_RETURN_RESUME_DONE \
+	TEESMC_CALL_VAL(TEESMC_64, TEESMC_FAST_CALL, \
+			TEESMC_OWNER_TRUSTED_OS_TKCORED, \
+			TEESMC_TKCORED_FUNCID_RETURN_RESUME_DONE)
+
+#define TEESMC_TKCORED_FUNCID_RETURN_CALL_DONE		5
+#define TEESMC32_TKCORED_RETURN_CALL_DONE \
+	TEESMC_CALL_VAL(TEESMC_32, TEESMC_FAST_CALL, \
+			TEESMC_OWNER_TRUSTED_OS_TKCORED, \
+			TEESMC_TKCORED_FUNCID_RETURN_CALL_DONE)
+#define TEESMC64_TKCORED_RETURN_CALL_DONE \
+	TEESMC_CALL_VAL(TEESMC_64, TEESMC_FAST_CALL, \
+			TEESMC_OWNER_TRUSTED_OS_TKCORED, \
+			TEESMC_TKCORED_FUNCID_RETURN_CALL_DONE)
+
+
+#define TEESMC_TKCORED_FUNCID_RETURN_FIQ_DONE		6
+#define TEESMC32_TKCORED_RETURN_FIQ_DONE \
+	TEESMC_CALL_VAL(TEESMC_32, TEESMC_FAST_CALL, \
+			TEESMC_OWNER_TRUSTED_OS_TKCORED, \
+			TEESMC_TKCORED_FUNCID_RETURN_FIQ_DONE)
+#define TEESMC64_TKCORED_RETURN_FIQ_DONE \
+	TEESMC_CALL_VAL(TEESMC_64, TEESMC_FAST_CALL, \
+			TEESMC_OWNER_TRUSTED_OS_TKCORED, \
+			TEESMC_TKCORED_FUNCID_RETURN_FIQ_DONE)
+
+
+#define TEESMC_TKCORED_FUNCID_SECURE_SERVICE_REQUEST	0x10
+#define TEESMC32_TKCORED_SECURE_SERVICE_REQUEST \
+	TEESMC_CALL_VAL(TEESMC_32, TEESMC_FAST_CALL, \
+			TEESMC_OWNER_TRUSTED_OS_TKCORED, \
+			TEESMC_TKCORED_FUNCID_SECURE_SERVICE_REQUEST)
+#define TEESMC64_TKCORED_SECURE_SERVICE_REQUEST \
+	TEESMC_CALL_VAL(TEESMC_64, TEESMC_FAST_CALL, \
+			TEESMC_OWNER_TRUSTED_OS_TKCORED, \
+			TEESMC_TKCORED_FUNCID_SECURE_SERVICE_REQUEST)
+
+#endif
diff --git a/services/spd/tkcored/tkcored.mk b/services/spd/tkcored/tkcored.mk
new file mode 100644
index 0000000..fce3e99
--- /dev/null
+++ b/services/spd/tkcored/tkcored.mk
@@ -0,0 +1,8 @@
+
+TKCORED_DIR		:=	services/spd/tkcored
+SPD_INCLUDES		:=	-Iinclude/bl32/tkcore
+
+SPD_SOURCES		:=	services/spd/tkcored/tkcored_common.c	\
+				services/spd/tkcored/tkcored_helpers.S	\
+				services/spd/tkcored/tkcored_main.c	\
+				services/spd/tkcored/tkcored_pm.c
diff --git a/services/spd/tkcored/tkcored_common.c b/services/spd/tkcored/tkcored_common.c
new file mode 100644
index 0000000..8238bf0
--- /dev/null
+++ b/services/spd/tkcored/tkcored_common.c
@@ -0,0 +1,110 @@
+#include <arch_helpers.h>
+#include <assert.h>
+#include <bl_common.h>
+#include <context_mgmt.h>
+#include <string.h>
+#include "tkcored_private.h"
+
+int32_t tkcored_init_secure_context(uint64_t entrypoint,
+		uint32_t rw,
+		uint64_t mpidr,
+		tkcore_context_t *tkcore_ctx)
+{
+	uint32_t scr, sctlr;
+	el1_sys_regs_t *el1_state;
+	uint32_t spsr;
+
+	/* Passing a NULL context is a critical programming error */
+	assert(tkcore_ctx);
+
+	/*
+	 * This might look redundant if the context was statically
+	 * allocated but this function cannot make that assumption.
+	 */
+	memset(tkcore_ctx, 0, sizeof(*tkcore_ctx));
+
+	/*
+	 * Set the right security state, register width and enable access to
+	 * the secure physical timer for TKCORE.
+	 */
+	scr = read_scr();
+	scr &= ~SCR_NS_BIT;
+	scr &= ~SCR_RW_BIT;
+	scr |= SCR_ST_BIT;
+	if (rw == TKCORE_AARCH64)
+		scr |= SCR_RW_BIT;
+
+	/* Get a pointer to the S-EL1 context memory */
+	el1_state = get_sysregs_ctx(&tkcore_ctx->cpu_ctx);
+
+	/*
+	 * Program the SCTLR_EL1 such that upon entry in S-EL1, caches and
+	 * MMU are disabled and exception endianess is set to be the same
+	 * as EL3
+	 */
+	sctlr = read_sctlr_el3();
+	sctlr &= SCTLR_EE_BIT;
+	sctlr |= SCTLR_EL1_RES1;
+	write_ctx_reg(el1_state, CTX_SCTLR_EL1, sctlr);
+
+	/* Set this context as ready to be initialised i.e OFF */
+	set_tkcore_pstate(tkcore_ctx->state, TKCORE_PSTATE_OFF);
+
+	/* Associate this context with the cpu specified */
+	tkcore_ctx->mpidr = mpidr;
+
+	cm_set_context_by_mpidr(mpidr, &tkcore_ctx->cpu_ctx, SECURE);
+	if (rw == TKCORE_AARCH64)
+		spsr = SPSR_64(MODE_EL1, MODE_SP_ELX, DISABLE_ALL_EXCEPTIONS);
+	else
+		spsr = SPSR_MODE32(MODE32_svc, SPSR_T_ARM, SPSR_E_LITTLE,
+			DAIF_FIQ_BIT | DAIF_IRQ_BIT);
+
+	//cm_set_el3_eret_context(SECURE, entrypoint, spsr, scr);
+	{
+
+		el3_state_t *state;
+		scr &= ~SCR_FIQ_BIT;
+		scr &= ~SCR_IRQ_BIT;
+		scr |= get_scr_el3_from_routing_model(SECURE);
+		state = get_el3state_ctx(&tkcore_ctx->cpu_ctx);
+		write_ctx_reg(state, CTX_SCR_EL3, scr);
+	}
+
+	cm_set_elr_spsr_el3(SECURE, entrypoint, spsr);
+
+	return 0;
+}
+
+uint64_t tkcored_synchronous_sp_entry(tkcore_context_t *tkcore_ctx)
+{
+	uint64_t rc;
+
+	assert(tkcore_ctx->c_rt_ctx == 0);
+
+	/* Apply the Secure EL1 system register context and switch to it */
+	assert(cm_get_context_by_mpidr(read_mpidr(), SECURE) == &tkcore_ctx->cpu_ctx);
+	cm_el1_sysregs_context_restore(SECURE);
+	cm_set_next_eret_context(SECURE);
+
+	rc = tkcored_enter_sp(&tkcore_ctx->c_rt_ctx);
+//#if DEBUG
+	tkcore_ctx->c_rt_ctx = 0;
+//#endif
+
+	return rc;
+}
+
+
+void tkcored_synchronous_sp_exit(tkcore_context_t *tkcore_ctx, uint64_t ret)
+{
+	/* Save the Secure EL1 system register context */
+	assert(cm_get_context_by_mpidr(read_mpidr(), SECURE) == &tkcore_ctx->cpu_ctx);
+	cm_el1_sysregs_context_save(SECURE);
+
+	assert(tkcore_ctx->c_rt_ctx != 0);
+	tkcored_exit_sp(tkcore_ctx->c_rt_ctx, ret);
+
+	/* Should never reach here */
+	assert(0);
+}
diff --git a/services/spd/tkcored/tkcored_helpers.S b/services/spd/tkcored/tkcored_helpers.S
new file mode 100644
index 0000000..216ba28
--- /dev/null
+++ b/services/spd/tkcored/tkcored_helpers.S
@@ -0,0 +1,45 @@
+#include <asm_macros.S>
+#include "tkcored_private.h"
+
+	.global	tkcored_enter_sp
+
+func tkcored_enter_sp
+	/* Make space for the registers that we're going to save */
+	mov	x3, sp
+	str	x3, [x0, #0]
+	sub	sp, sp, #TKCORED_C_RT_CTX_SIZE
+
+	/* Save callee-saved registers on to the stack */
+	stp	x19, x20, [sp, #TKCORED_C_RT_CTX_X19]
+	stp	x21, x22, [sp, #TKCORED_C_RT_CTX_X21]
+	stp	x23, x24, [sp, #TKCORED_C_RT_CTX_X23]
+	stp	x25, x26, [sp, #TKCORED_C_RT_CTX_X25]
+	stp	x27, x28, [sp, #TKCORED_C_RT_CTX_X27]
+	stp	x29, x30, [sp, #TKCORED_C_RT_CTX_X29]
+
+	mrs	x19, scr_el3
+	stp	x19, xzr, [sp, #TKCORED_C_RT_CTX_SCR]
+
+	b	el3_exit
+endfunc tkcored_enter_sp
+
+	.global tkcored_exit_sp
+func tkcored_exit_sp
+	/* Restore the previous stack */
+	mov	sp, x0
+
+	/* Restore scr_el3 */
+	ldp	x19, xzr, [sp, #(TKCORED_C_RT_CTX_SCR - TKCORED_C_RT_CTX_SIZE)]
+	msr	scr_el3, x19
+
+	/* Restore callee-saved registers on to the stack */
+	ldp	x19, x20, [x0, #(TKCORED_C_RT_CTX_X19 - TKCORED_C_RT_CTX_SIZE)]
+	ldp	x21, x22, [x0, #(TKCORED_C_RT_CTX_X21 - TKCORED_C_RT_CTX_SIZE)]
+	ldp	x23, x24, [x0, #(TKCORED_C_RT_CTX_X23 - TKCORED_C_RT_CTX_SIZE)]
+	ldp	x25, x26, [x0, #(TKCORED_C_RT_CTX_X25 - TKCORED_C_RT_CTX_SIZE)]
+	ldp	x27, x28, [x0, #(TKCORED_C_RT_CTX_X27 - TKCORED_C_RT_CTX_SIZE)]
+	ldp	x29, x30, [x0, #(TKCORED_C_RT_CTX_X29 - TKCORED_C_RT_CTX_SIZE)]
+
+	mov	x0, x1
+	ret
+endfunc tkcored_exit_sp
diff --git a/services/spd/tkcored/tkcored_main.c b/services/spd/tkcored/tkcored_main.c
new file mode 100644
index 0000000..c18d093
--- /dev/null
+++ b/services/spd/tkcored/tkcored_main.c
@@ -0,0 +1,488 @@
+#include <arch_helpers.h>
+#include <assert.h>
+#include <bl_common.h>
+#include <bl31.h>
+#include <context_mgmt.h>
+#include <debug.h>
+#include <errno.h>
+#include <platform.h>
+#include <runtime_svc.h>
+#include <stddef.h>
+#include <console.h>
+#include <tkcore.h>
+#include <uuid.h>
+#include "plat_private.h"
+#include "tkcored_private.h"
+#include "tkcore.h"
+#include "teesmc.h"
+#include "teesmc_tkcored.h"
+
+#if (defined(MACH_TYPE_MT6735) || defined(MACH_TYPE_MT6735M) || \
+	defined(MACH_TYPE_MT6753) || defined(MACH_TYPE_MT8173))
+#define TKCORE_BOOT_ARG_ADDR (((struct atf_arg_t *)(uintptr_t) TEE_BOOT_INFO_ADDR)->tee_boot_arg_addr)
+#else
+#define TKCORE_BOOT_ARG_ADDR (((struct atf_arg_t *)(uintptr_t) (&gteearg))->tee_boot_arg_addr)
+#endif
+
+tkcore_vectors_t *tkcore_vectors;
+
+tkcore_context_t tkcored_sp_context[TKCORED_CORE_COUNT];
+uint32_t tkcored_rw;
+
+//static uint64_t tkcoreBaseCoreMpidr;
+
+static int32_t tkcored_init(void);
+
+static void dump_caller_state(void *handle)
+{
+	uint64_t elr_el3, spsr_el3, sp_el1;
+
+	elr_el3 = SMC_GET_EL3(handle, CTX_ELR_EL3);
+	spsr_el3 = SMC_GET_EL3(handle, CTX_SPSR_EL3);
+	sp_el1 = read_ctx_reg(get_sysregs_ctx(handle), CTX_SP_EL1);
+
+	tf_printf("ELR_EL3: 0x%016lx\tSPSR_EL3: 0x%016lx\tSP_EL1: 0x%016lx\n",
+		elr_el3, spsr_el3, sp_el1);
+
+	tf_printf("X0: 0x%016lx\tX1: 0x%016lx\tX2: 0x%016lx\tX3: 0x%016lx\n",
+		read_ctx_reg(get_gpregs_ctx(handle), CTX_GPREG_X0),
+		read_ctx_reg(get_gpregs_ctx(handle), CTX_GPREG_X1),
+		read_ctx_reg(get_gpregs_ctx(handle), CTX_GPREG_X2),
+		read_ctx_reg(get_gpregs_ctx(handle), CTX_GPREG_X3));
+
+	tf_printf("X4: 0x%016lx\tX5: 0x%016lx\tX6: 0x%016lx\tX7: 0x%016lx\n",
+		read_ctx_reg(get_gpregs_ctx(handle), CTX_GPREG_X4),
+		read_ctx_reg(get_gpregs_ctx(handle), CTX_GPREG_X5),
+		read_ctx_reg(get_gpregs_ctx(handle), CTX_GPREG_X6),
+		read_ctx_reg(get_gpregs_ctx(handle), CTX_GPREG_X7));
+
+	tf_printf("X8: 0x%016lx\tX9: 0x%016lx\tX10: 0x%016lx\tX11: 0x%016lx\n",
+		read_ctx_reg(get_gpregs_ctx(handle), CTX_GPREG_X8),
+		read_ctx_reg(get_gpregs_ctx(handle), CTX_GPREG_X9),
+		read_ctx_reg(get_gpregs_ctx(handle), CTX_GPREG_X10),
+		read_ctx_reg(get_gpregs_ctx(handle), CTX_GPREG_X11));
+
+	tf_printf("X12: 0x%016lx\tX13: 0x%016lx\tX14: 0x%016lx\tX15: 0x%016lx\n",
+		read_ctx_reg(get_gpregs_ctx(handle), CTX_GPREG_X12),
+		read_ctx_reg(get_gpregs_ctx(handle), CTX_GPREG_X13),
+		read_ctx_reg(get_gpregs_ctx(handle), CTX_GPREG_X14),
+		read_ctx_reg(get_gpregs_ctx(handle), CTX_GPREG_X15));
+
+	tf_printf("X16: 0x%016lx\tX17: 0x%016lx\tX18: 0x%016lx\tX19: 0x%016lx\n",
+		read_ctx_reg(get_gpregs_ctx(handle), CTX_GPREG_X16),
+		read_ctx_reg(get_gpregs_ctx(handle), CTX_GPREG_X17),
+		read_ctx_reg(get_gpregs_ctx(handle), CTX_GPREG_X18),
+		read_ctx_reg(get_gpregs_ctx(handle), CTX_GPREG_X19));
+
+	tf_printf("X20: 0x%016lx\tX21: 0x%016lx\tX22: 0x%016lx\tX23: 0x%016lx\n",
+		read_ctx_reg(get_gpregs_ctx(handle), CTX_GPREG_X20),
+		read_ctx_reg(get_gpregs_ctx(handle), CTX_GPREG_X21),
+		read_ctx_reg(get_gpregs_ctx(handle), CTX_GPREG_X22),
+		read_ctx_reg(get_gpregs_ctx(handle), CTX_GPREG_X23));
+
+	tf_printf("X24: 0x%016lx\tX25: 0x%016lx\tX26: 0x%016lx\tX27: 0x%016lx\n",
+		read_ctx_reg(get_gpregs_ctx(handle), CTX_GPREG_X24),
+		read_ctx_reg(get_gpregs_ctx(handle), CTX_GPREG_X25),
+		read_ctx_reg(get_gpregs_ctx(handle), CTX_GPREG_X26),
+		read_ctx_reg(get_gpregs_ctx(handle), CTX_GPREG_X27));
+
+	tf_printf("X28: 0x%016lx\tX29: 0x%016lx\tLR: 0x%016lx\tSP_EL0: 0x%016lx\n",
+		read_ctx_reg(get_gpregs_ctx(handle), CTX_GPREG_X28),
+		read_ctx_reg(get_gpregs_ctx(handle), CTX_GPREG_X29),
+		read_ctx_reg(get_gpregs_ctx(handle), CTX_GPREG_LR),
+		read_ctx_reg(get_gpregs_ctx(handle), CTX_GPREG_SP_EL0));
+}
+
+static uint64_t tkcored_sel1_interrupt_handler(uint32_t id,
+						uint32_t flags,
+						void *handle,
+						void *cookie)
+{
+	unsigned int linear_id;
+	tkcore_context_t *tkcore_ctx;
+	uint32_t irq = id & 0x3ff;
+
+	/* Check the security state when the exception was generated
+	   FIQ for swd will be directly directed to EL1 for GICv2. */
+	assert(get_interrupt_src_ss(flags) == NON_SECURE);
+
+#if IMF_READ_INTERRUPT_ID
+	/* Check the security status of the interrupt */
+	assert(plat_ic_get_interrupt_type(id) == INTR_TYPE_S_EL1);
+#endif
+
+	/* Sanity check the pointer to this cpu's context */
+	linear_id = platform_get_core_pos(read_mpidr());
+	assert(handle == cm_get_context_by_index(linear_id, NON_SECURE));
+
+	if (irq == FIQ_SMP_CALL_SGI) {
+
+		console_init(gteearg.atf_log_port, UART_CLOCK, UART_BAUDRATE);
+
+		tf_printf("##### %s ACK PLAT_SMP_CALL_SGI. Dump State. ###### \n", __func__);
+
+		plat_ic_acknowledge_interrupt();
+		plat_ic_end_of_interrupt(irq);
+
+		plat_tkcore_dump();
+	} else if (irq == WDT_IRQ_BIT_ID) {
+		if (plat_ic_acknowledge_interrupt() == WDT_IRQ_BIT_ID) {
+
+			console_init(gteearg.atf_log_port, UART_CLOCK, UART_BAUDRATE);
+			tf_printf("##### %s ACK WDT_IRQ. Redirecting to TEE... ###### \n", __func__);
+
+			/* which means this core successfully ack WDT IRQ ahead of all other cores.
+				and others won't able to consume this irq any more. */
+			plat_tkcore_secure_service_request(PLAT_TKCORE_SECURE_SERVICE_WDT_DUMP, NULL, NULL);
+
+			tf_printf("##### %s ACK WDT_IRQ. Returns from TEE... ###### \n", __func__);
+
+			plat_ic_end_of_interrupt(irq);
+		} else {
+
+			console_init(gteearg.atf_log_port, UART_CLOCK, UART_BAUDRATE);
+
+			tf_printf("##### %s WDT_IRQ already ACKED. ###### \n", __func__);
+			tf_printf("##### %s Return to NS-EL1. ###### \n", __func__);
+
+			SMC_RET0(handle);
+		}
+	} else {
+		console_init(gteearg.atf_log_port, UART_CLOCK, UART_BAUDRATE);
+
+		tf_printf("##### %s UNKNOWN SEL1-IRQ ID %u ##### \n", __func__, irq);
+		dump_caller_state(handle);
+
+		/* Other GROUP1-interrupt will be redirected to tkcore */
+		/* Currently S-EL1 irq cannot be read by ATF. But we know it's a WDT */
+		plat_tkcore_secure_service_request(PLAT_TKCORE_SECURE_SERVICE_WDT_DUMP, NULL, NULL);
+
+		/* Save the non-secure context before entering the TKCORE */
+		cm_el1_sysregs_context_save(NON_SECURE);
+
+		/* Get a reference to this cpu's TKCORE context */
+		tkcore_ctx = &tkcored_sp_context[linear_id];
+		assert(&tkcore_ctx->cpu_ctx == cm_get_context_by_index(linear_id, SECURE));
+
+#if 0
+	SMC_SET_EL3(&tkcore_ctx->cpu_ctx,
+			CTX_SPSR_EL3,
+			SPSR_64(MODE_EL1, MODE_SP_ELX, DISABLE_ALL_EXCEPTIONS));
+#endif
+		SMC_SET_EL3(&tkcore_ctx->cpu_ctx,
+				CTX_ELR_EL3,
+				(uint64_t)&tkcore_vectors->fiq_entry);
+		cm_el1_sysregs_context_restore(SECURE);
+		cm_set_next_eret_context(SECURE);
+
+		SMC_RET2(&tkcore_ctx->cpu_ctx, TKCORE_HANDLE_FIQ_AND_RETURN, read_elr_el3());
+	}
+
+	return 0;
+}
+
+// Core-specific context initialization for non-primary cores
+void tkcored_init_core(uint64_t mpidr) {
+#if 0
+	uint32_t linear_id = platform_get_core_pos(mpidr);
+	tkcore_context_t *tkcore_ctx = &tkcored_sp_context[linear_id];
+	uint32_t boot_core_nro;
+
+	tbase_init_secure_context(tbase_ctx);
+
+	boot_core_nro = platform_get_core_pos(tkcoreBootCoreMpidr);
+	save_sysregs_core(boot_core_nro, linear_id);
+#endif
+}
+
+int32_t tkcored_setup(void)
+{
+	entry_point_info_t *image_info;
+	int32_t rc;
+	uint64_t mpidr = read_mpidr();
+	uint32_t linear_id;
+
+	linear_id = platform_get_core_pos(mpidr);
+
+	/*
+	 * Get information about the Secure Payload (BL32) image. Its
+	 * absence is a critical failure.  TODO: Add support to
+	 * conditionally include the SPD service
+	 */
+	image_info = bl31_plat_get_next_image_ep_info(SECURE);
+	assert(image_info);
+
+	/*
+	 * If there's no valid entry point for SP, we return a non-zero value
+	 * signalling failure initializing the service. We bail out without
+	 * registering any handlers
+	 */
+	if (!image_info->pc)
+		return 1;
+
+	/*
+	 * We could inspect the SP image and determine it's execution
+	 * state i.e whether AArch32 or AArch64. Assuming it's AArch32
+	 * for the time being.
+	 */
+	tkcored_rw = TKCORE_AARCH32;
+	rc = tkcored_init_secure_context(image_info->pc, tkcored_rw,
+						mpidr, &tkcored_sp_context[linear_id]);
+	assert(rc == 0);
+
+	/*
+	 * All TKCORED initialization done. Now register our init function with
+	 * BL31 for deferred invocation
+	 */
+	bl31_register_bl32_init(&tkcored_init);
+
+	return rc;
+}
+
+static int32_t tkcored_init(void)
+{
+	uint64_t mpidr = read_mpidr();
+	uint32_t linear_id = platform_get_core_pos(mpidr), flags;
+	uint64_t rc;
+	tkcore_context_t *tkcore_ctx = &tkcored_sp_context[linear_id];
+
+	// Save el1 registers in case non-secure world has already been set up.
+	cm_el1_sysregs_context_save(NON_SECURE);
+
+	/*
+	 * Arrange for an entry into the test secure payload. We expect an array
+	 * of vectors in return
+	 */
+
+	write_ctx_reg(get_gpregs_ctx(&(tkcore_ctx->cpu_ctx)), CTX_GPREG_X0,
+		(int64_t) TKCORE_BOOT_ARG_ADDR );
+	tf_printf("TKCore Entry %016lx\n",
+		read_ctx_reg(get_el3state_ctx(&(tkcore_ctx->cpu_ctx)), CTX_ELR_EL3));
+	rc = tkcored_synchronous_sp_entry(tkcore_ctx);
+	tf_printf("TKCore Exit\n");
+	assert(rc != 0);
+	if (rc) {
+		set_tkcore_pstate(tkcore_ctx->state, TKCORE_PSTATE_ON);
+
+		/*
+		 * TKCORE has been successfully initialized. Register power
+		 * managemnt hooks with PSCI
+		 */
+		psci_register_spd_pm_hook(&tkcored_pm);
+	}
+
+	/*
+	 * Register an interrupt handler for S-EL1 interrupts when generated
+	 * during code executing in the non-secure state.
+	 */
+	flags = 0;
+	set_interrupt_rm_flag(flags, NON_SECURE);
+	rc = register_interrupt_type_handler(INTR_TYPE_S_EL1,
+		tkcored_sel1_interrupt_handler, flags);
+	if (rc)
+		panic();
+
+	cm_el1_sysregs_context_restore(NON_SECURE);
+
+	return rc;
+}
+
+uint64_t tkcored_smc_handler(uint32_t smc_fid,
+			uint64_t x1,
+			uint64_t x2,
+			uint64_t x3,
+			uint64_t x4,
+			void *cookie,
+			void *handle,
+			uint64_t flags)
+{
+	cpu_context_t *ns_cpu_context;
+	unsigned long mpidr = read_mpidr();
+	uint32_t linear_id = platform_get_core_pos(mpidr);
+	tkcore_context_t *tkcore_ctx = &tkcored_sp_context[linear_id];
+
+	/*
+	 * Determine which security state this SMC originated from
+	 */
+
+	if (is_caller_non_secure(flags)) {
+		assert(handle == cm_get_context_by_index(linear_id, NON_SECURE));
+
+		cm_el1_sysregs_context_save(NON_SECURE);
+
+		/*
+		 * Verify if there is a valid context to use, copy the
+		 * operation type and parameters to the secure context
+		 * and jump to the fast smc entry point in the secure
+		 * payload. Entry into S-EL1 will take place upon exit
+		 * from this function.
+		 */
+		assert(&tkcore_ctx->cpu_ctx ==
+			cm_get_context_by_index(linear_id, SECURE));
+
+		/* Set appropriate entry for SMC.
+		 * We expect TKCORE to manage the PSTATE.I and PSTATE.F
+		 * flags as appropriate.
+		 */
+		if (GET_SMC_TYPE(smc_fid) == SMC_TYPE_FAST) {
+			cm_set_elr_el3(SECURE, (uint64_t)
+					&tkcore_vectors->fast_smc_entry);
+		} else {
+			cm_set_elr_el3(SECURE, (uint64_t)
+					&tkcore_vectors->std_smc_entry);
+		}
+
+		cm_el1_sysregs_context_restore(SECURE);
+		cm_set_next_eret_context(SECURE);
+
+		/* Propagate hypervisor client ID */
+		write_ctx_reg(get_gpregs_ctx(&tkcore_ctx->cpu_ctx),
+			CTX_GPREG_X7, read_ctx_reg(get_gpregs_ctx(handle),
+			CTX_GPREG_X7));
+
+		SMC_RET4(&tkcore_ctx->cpu_ctx, smc_fid, x1, x2, x3);
+	}
+
+	/*
+	 * Returning from TKCORE
+	 */
+
+	switch (smc_fid) {
+	/*
+	 * TKCORE has finished initialising itself after a cold boot
+	 */
+	case TEESMC32_TKCORED_RETURN_ENTRY_DONE:
+	case TEESMC64_TKCORED_RETURN_ENTRY_DONE:
+		/*
+		* Stash the TKCORE entry points information. This is done
+		* only once on the primary cpu
+		*/
+		assert(tkcore_vectors == NULL);
+		tkcore_vectors = (tkcore_vectors_t *) x1;
+
+		/*
+		 * TKCORE reports completion. The TKCORED must have initiated
+		 * the original request through a synchronous entry into
+		 * the SP. Jump back to the original C runtime context.
+		 */
+		tkcored_synchronous_sp_exit(tkcore_ctx, x1);
+
+
+	/*
+	 * TKCORE has finished turning itself on in response to an earlier
+	 * psci cpu_on request
+	 */
+	case TEESMC32_TKCORED_RETURN_ON_DONE:
+	case TEESMC64_TKCORED_RETURN_ON_DONE:
+
+	/*
+	 * TKCORE has finished turning itself off in response to an earlier
+	 * psci cpu_off request.
+	 */
+	case TEESMC32_TKCORED_RETURN_OFF_DONE:
+	case TEESMC64_TKCORED_RETURN_OFF_DONE:
+	/*
+	 * TKCORE has finished resuming itself after an earlier psci
+	 * cpu_suspend request.
+	 */
+	case TEESMC32_TKCORED_RETURN_RESUME_DONE:
+	case TEESMC64_TKCORED_RETURN_RESUME_DONE:
+	/*
+	 * TKCORE has finished suspending itself after an earlier psci
+	 * cpu_suspend request.
+	 */
+	case TEESMC32_TKCORED_RETURN_SUSPEND_DONE:
+	case TEESMC64_TKCORED_RETURN_SUSPEND_DONE:
+
+		/*
+		 * TKCORE reports completion. The TKCORED must have initiated the
+		 * original request through a synchronous entry into TKCORE.
+		 * Jump back to the original C runtime context, and pass x1 as
+		 * return value to the caller
+		 */
+		tkcored_synchronous_sp_exit(tkcore_ctx, x1);
+
+	/*
+	 * TKCORE is returning from a call or being preemted from a call, in
+	 * either case execution should resume in the normal world.
+	 */
+	case TEESMC32_TKCORED_RETURN_CALL_DONE:
+	case TEESMC64_TKCORED_RETURN_CALL_DONE:
+		/*
+		 * This is the result from the secure client of an
+		 * earlier request. The results are in x0-x3. Copy it
+		 * into the non-secure context, save the secure state
+		 * and return to the non-secure state.
+		 */
+		assert(handle == cm_get_context_by_index(linear_id, SECURE));
+
+		cm_el1_sysregs_context_save(SECURE);
+
+		/* Get a reference to the non-secure context */
+		ns_cpu_context = cm_get_context_by_index(linear_id, NON_SECURE);
+		assert(ns_cpu_context);
+
+		/* Restore non-secure state */
+		cm_el1_sysregs_context_restore(NON_SECURE);
+		cm_set_next_eret_context(NON_SECURE);
+
+		x1 &= ((1UL << 32) - 1);
+		x2 &= ((1UL << 32) - 1);
+		x3 &= ((1UL << 32) - 1);
+		x4 &= ((1UL << 32) - 1);
+
+		SMC_RET4(ns_cpu_context, x1, x2, x3, x4);
+
+	/*
+	 * TKCORE has finished handling a S-EL1 FIQ interrupt. Execution
+	 * should resume in the normal world.
+	 */
+	case TEESMC32_TKCORED_RETURN_FIQ_DONE:
+	case TEESMC64_TKCORED_RETURN_FIQ_DONE:
+		/* Get a reference to the non-secure context */
+		ns_cpu_context = cm_get_context_by_index(linear_id, NON_SECURE);
+		assert(ns_cpu_context);
+
+		/*
+		 * Restore non-secure state. There is no need to save the
+		 * secure system register context since TKCORE was supposed
+		 * to preserve it during S-EL1 interrupt handling.
+		 */
+		cm_el1_sysregs_context_restore(NON_SECURE);
+		cm_set_next_eret_context(NON_SECURE);
+
+		SMC_RET0((uint64_t) ns_cpu_context);
+
+	case TEESMC32_TKCORED_SECURE_SERVICE_REQUEST:
+	case TEESMC64_TKCORED_SECURE_SERVICE_REQUEST:
+
+		smc_fid = plat_tkcore_secure_service_request(x1, &x2, &x3);
+
+		SMC_RET4(&tkcore_ctx->cpu_ctx, smc_fid, x1, x2, x3);
+
+	default:
+		assert(0);
+	}
+}
+
+/* Define an TKCORED runtime service descriptor for fast SMC calls */
+DECLARE_RT_SVC(
+	tkcored_fast,
+
+	OEN_TOS_START,
+	OEN_TOS_END,
+	SMC_TYPE_FAST,
+	tkcored_setup,
+	tkcored_smc_handler
+);
+
+/* Define an TKCORED runtime service descriptor for standard SMC calls */
+DECLARE_RT_SVC(
+	tkcored_std,
+
+	OEN_TOS_START,
+	OEN_TOS_END,
+	SMC_TYPE_STD,
+	NULL,
+	tkcored_smc_handler
+);
diff --git a/services/spd/tkcored/tkcored_pm.c b/services/spd/tkcored/tkcored_pm.c
new file mode 100644
index 0000000..c921377
--- /dev/null
+++ b/services/spd/tkcored/tkcored_pm.c
@@ -0,0 +1,153 @@
+#include <arch_helpers.h>
+#include <assert.h>
+#include <bl_common.h>
+#include <context_mgmt.h>
+#include <debug.h>
+#include <platform.h>
+#include <tkcore.h>
+#include "tkcored_private.h"
+
+static void tkcored_cpu_on_handler(uint64_t target_cpu)
+{
+
+	//uint64_t mpidr = read_mpidr();
+	uint32_t linear_id = platform_get_core_pos(target_cpu);
+
+	tkcore_context_t *tkcore_ctx = &tkcored_sp_context[linear_id];
+
+	set_tkcore_pstate(tkcore_ctx->state, TKCORE_PSTATE_ON_PENDING);
+}
+
+static int32_t tkcored_cpu_off_handler(uint64_t cookie)
+{
+	int32_t rc = 0;
+	uint64_t mpidr = read_mpidr();
+	uint32_t linear_id = platform_get_core_pos(mpidr);
+
+	tkcore_context_t *tkcore_ctx = &tkcored_sp_context[linear_id];
+
+	assert(tkcore_vectors);
+	assert(get_tkcore_pstate(tkcore_ctx->state) == TKCORE_PSTATE_ON);
+
+	/* Program the entry point and enter TKCORE */
+	cm_set_elr_el3(SECURE, (uint64_t) &tkcore_vectors->cpu_off_entry);
+	rc = tkcored_synchronous_sp_entry(tkcore_ctx);
+
+	/*
+	 * Read the response from TKCORE. A non-zero return means that
+	 * something went wrong while communicating with TKCORE.
+	 */
+	if (rc != 0)
+		panic();
+
+	/*
+	 * Reset TKCORE's context for a fresh start when this cpu is turned on
+	 * subsequently.
+	 */
+	set_tkcore_pstate(tkcore_ctx->state, TKCORE_PSTATE_OFF);
+
+	 return 0;
+}
+
+static void tkcored_cpu_suspend_handler(uint64_t power_state)
+{
+	int32_t rc = 0;
+	uint64_t mpidr = read_mpidr();
+	uint32_t linear_id = platform_get_core_pos(mpidr);
+	tkcore_context_t *tkcore_ctx = &tkcored_sp_context[linear_id];
+
+	assert(tkcore_vectors);
+	assert(get_tkcore_pstate(tkcore_ctx->state) == TKCORE_PSTATE_ON);
+
+	/* Program the entry point, power_state parameter and enter TKCORE */
+	write_ctx_reg(get_gpregs_ctx(&tkcore_ctx->cpu_ctx),
+		      CTX_GPREG_X0,
+		      power_state);
+	cm_set_elr_el3(SECURE, (uint64_t) &tkcore_vectors->cpu_suspend_entry);
+	rc = tkcored_synchronous_sp_entry(tkcore_ctx);
+
+	/*
+	 * Read the response from TKCORE. A non-zero return means that
+	 * something went wrong while communicating with TKCORE.
+	 */
+	if (rc != 0)
+		panic();
+
+	/* Update its context to reflect the state TKCORE is in */
+	set_tkcore_pstate(tkcore_ctx->state, TKCORE_PSTATE_SUSPEND);
+}
+
+static void tkcored_cpu_on_finish_handler(uint64_t cookie)
+{
+	int32_t rc = 0;
+	uint64_t mpidr = read_mpidr();
+	uint32_t linear_id = platform_get_core_pos(mpidr);
+	tkcore_context_t *tkcore_ctx = &tkcored_sp_context[linear_id];
+
+	assert(tkcore_vectors);
+	assert(get_tkcore_pstate(tkcore_ctx->state) == TKCORE_PSTATE_ON_PENDING);
+
+	//tkcored_init_core(mpidr);
+
+	/* Initialise this cpu's secure context */
+	tkcored_init_secure_context((uint64_t)&tkcore_vectors->cpu_on_entry,
+				   tkcored_rw, mpidr, tkcore_ctx);
+
+	/* Enter TKCORE */
+	rc = tkcored_synchronous_sp_entry(tkcore_ctx);
+
+	/*
+	 * Read the response from TKCORE. A non-zero return means that
+	 * something went wrong while communicating with TKCORE.
+	 */
+	if (rc != 0)
+		panic();
+
+	/* Update its context to reflect the state TKCORE is in */
+	set_tkcore_pstate(tkcore_ctx->state, TKCORE_PSTATE_ON);
+}
+
+static void tkcored_cpu_suspend_finish_handler(uint64_t suspend_level)
+{
+	int32_t rc = 0;
+	uint64_t mpidr = read_mpidr();
+	uint32_t linear_id = platform_get_core_pos(mpidr);
+
+	tkcore_context_t *tkcore_ctx = &tkcored_sp_context[linear_id];
+
+	assert(tkcore_vectors);
+	assert(get_tkcore_pstate(tkcore_ctx->state) == TKCORE_PSTATE_SUSPEND);
+
+	/* Program the entry point, suspend_level and enter the SP */
+	write_ctx_reg(get_gpregs_ctx(&tkcore_ctx->cpu_ctx),
+		      CTX_GPREG_X0,
+		      suspend_level);
+	cm_set_elr_el3(SECURE, (uint64_t) &tkcore_vectors->cpu_resume_entry);
+	rc = tkcored_synchronous_sp_entry(tkcore_ctx);
+
+	/*
+	 * Read the response from TKCORE. A non-zero return means that
+	 * something went wrong while communicating with TKCORE.
+	 */
+	if (rc != 0)
+		panic();
+
+	/* Update its context to reflect the state TKCORE is in */
+	set_tkcore_pstate(tkcore_ctx->state, TKCORE_PSTATE_ON);
+}
+
+static int32_t tkcored_cpu_migrate_info(uint64_t *resident_cpu)
+{
+	return TKCORE_MIGRATE_INFO;
+}
+
+const spd_pm_ops_t tkcored_pm = {
+	tkcored_cpu_on_handler,
+	tkcored_cpu_off_handler,
+	tkcored_cpu_suspend_handler,
+	tkcored_cpu_on_finish_handler,
+	tkcored_cpu_suspend_finish_handler,
+	NULL,
+	tkcored_cpu_migrate_info
+};
+
diff --git a/services/spd/tkcored/tkcored_private.h b/services/spd/tkcored/tkcored_private.h
new file mode 100644
index 0000000..91e2c47
--- /dev/null
+++ b/services/spd/tkcored/tkcored_private.h
@@ -0,0 +1,119 @@
+
+#ifndef __TKCORED_PRIVATE_H__
+#define __TKCORED_PRIVATE_H__
+
+#include <arch.h>
+#include <context.h>
+#include <interrupt_mgmt.h>
+#include <platform_def.h>
+#include <psci.h>
+
+/*******************************************************************************
+ * TKCORE PM state information e.g. TKCORE is suspended, uninitialised etc
+ * and macros to access the state information in the per-cpu 'state' flags
+ ******************************************************************************/
+#define TKCORE_PSTATE_OFF		0
+#define TKCORE_PSTATE_ON			1
+#define TKCORE_PSTATE_SUSPEND	2
+#define TKCORE_PSTATE_ON_PENDING	4
+
+#define TKCORE_PSTATE_SHIFT		0
+#define TKCORE_PSTATE_MASK		0x7
+
+#define get_tkcore_pstate(state)	((state >> TKCORE_PSTATE_SHIFT) & \
+				 TKCORE_PSTATE_MASK)
+#define clr_tkcore_pstate(state)	(state &= ~(TKCORE_PSTATE_MASK \
+					    << TKCORE_PSTATE_SHIFT))
+#define set_tkcore_pstate(st, pst) do {					       \
+					clr_tkcore_pstate(st);		       \
+					st |= (pst & TKCORE_PSTATE_MASK) <<     \
+						TKCORE_PSTATE_SHIFT;	       \
+				} while (0)
+
+
+#define TKCORE_AARCH32		MODE_RW_32
+#define TKCORE_AARCH64		MODE_RW_64
+
+#define TKCORE_TYPE_UP		PSCI_TOS_NOT_UP_MIG_CAP
+#define TKCORE_TYPE_UPM		PSCI_TOS_UP_MIG_CAP
+#define TKCORE_TYPE_MP		PSCI_TOS_NOT_PRESENT_MP
+
+#define TKCORE_MIGRATE_INFO		TKCORE_TYPE_MP
+
+#define TKCORED_CORE_COUNT		PLATFORM_CORE_COUNT
+
+#define TKCORED_C_RT_CTX_X19		0x0
+#define TKCORED_C_RT_CTX_X20		0x8
+#define TKCORED_C_RT_CTX_X21		0x10
+#define TKCORED_C_RT_CTX_X22		0x18
+#define TKCORED_C_RT_CTX_X23		0x20
+#define TKCORED_C_RT_CTX_X24		0x28
+#define TKCORED_C_RT_CTX_X25		0x30
+#define TKCORED_C_RT_CTX_X26		0x38
+#define TKCORED_C_RT_CTX_X27		0x40
+#define TKCORED_C_RT_CTX_X28		0x48
+#define TKCORED_C_RT_CTX_X29		0x50
+#define TKCORED_C_RT_CTX_X30		0x58
+#define TKCORED_C_RT_CTX_SCR		0x60
+#define TKCORED_C_RT_CTX_SIZE		0x70
+#define TKCORED_C_RT_CTX_ENTRIES		(TKCORED_C_RT_CTX_SIZE >> DWORD_SHIFT)
+
+#ifndef __ASSEMBLY__
+
+#include <cassert.h>
+#include <stdint.h>
+
+#define TKCORE_NUM_ARGS	0x2
+
+DEFINE_REG_STRUCT(c_rt_regs, TKCORED_C_RT_CTX_ENTRIES);
+
+CASSERT(TKCORED_C_RT_CTX_SIZE == sizeof(c_rt_regs_t),	\
+	assert_spd_c_rt_regs_size_mismatch);
+
+/*******************************************************************************
+ * Structure which helps the TKCORED to maintain the per-cpu state of TKCORE.
+ * 'state'          - collection of flags to track TKCORE state e.g. on/off
+ * 'mpidr'          - mpidr to associate a context with a cpu
+ * 'c_rt_ctx'       - stack address to restore C runtime context from after
+ *                    returning from a synchronous entry into TKCORE.
+ * 'cpu_ctx'        - space to maintain TKCORE architectural state
+ ******************************************************************************/
+typedef struct tkcore_context {
+	uint32_t state;
+	uint64_t mpidr;
+	uint64_t c_rt_ctx;
+	cpu_context_t cpu_ctx;
+} tkcore_context_t;
+
+/* TKCORED power management handlers */
+extern const spd_pm_ops_t tkcored_pm;
+
+/*******************************************************************************
+ * Forward declarations
+ ******************************************************************************/
+struct tkcore_vectors;
+
+/*******************************************************************************
+ * Function & Data prototypes
+ ******************************************************************************/
+uint64_t tkcored_enter_sp(uint64_t *c_rt_ctx);
+void __dead2 tkcored_exit_sp(uint64_t c_rt_ctx, uint64_t ret);
+uint64_t tkcored_synchronous_sp_entry(tkcore_context_t *tkcore_ctx);
+void __dead2 tkcored_synchronous_sp_exit(tkcore_context_t *tkcore_ctx, uint64_t ret);
+int32_t tkcored_init_secure_context(uint64_t entrypoint,
+					uint32_t rw,
+					uint64_t mpidr,
+					tkcore_context_t *tkcore_ctx);
+extern tkcore_context_t tkcored_sp_context[TKCORED_CORE_COUNT];
+extern uint32_t tkcored_rw;
+extern struct tkcore_vectors *tkcore_vectors;
+
+#if DEBUG
+  #define DBG_PRINTF(...) tf_printf(__VA_ARGS__)
+#else
+  #define DBG_PRINTF(...)
+#endif
+
+#endif /*__ASSEMBLY__*/
+
+#endif /* __TKCORED_PRIVATE_H__ */
